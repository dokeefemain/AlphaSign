{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwvnXbYWXIea",
    "outputId": "dd674b7e-730b-4c09-ef30-329a42572b7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  2 22:17:46 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQzntgF6wcgj",
    "outputId": "7c2cfd46-776c-4e8d-99c7-74d19830afc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xtAuPPJw6_H",
    "outputId": "984ce95d-8080-48f5-e161-047a3f3c2b0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  drive/My Drive/datasets/train1.zip/train1.zip\n",
      "   creating: train1/\n",
      "  inflating: train1/X_train1.npy     \n",
      "  inflating: train1/X_val1.npy       \n",
      "  inflating: train1/y_train_e1.npy   \n",
      "  inflating: train1/y_val_e1.npy     \n"
     ]
    }
   ],
   "source": [
    "!unzip drive/My\\ Drive/datasets/train1.zip/train1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIgRBrbSxyy7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5hmGnl-x38I"
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"train1/X_train1.npy\")[1000:]\n",
    "X_val = np.load(\"train1/X_val1.npy\")[1000:]\n",
    "y_train_e = np.load(\"train1/y_train_e1.npy\")[1000:]\n",
    "y_val_e = np.load(\"train1/y_val_e1.npy\" )[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0ieT8cxyGH1"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, MaxPool2D, UpSampling2D, AveragePooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, Flatten, Dense, Add, Multiply, BatchNormalization, Dropout, concatenate\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "class IRV2():\n",
    "    def __init__(self, input_shape, n_classes, activation, p=1, t=2, r=1, scale1=0.17, scale2=0.1, scale3=0.2):\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.activation = activation\n",
    "        self.scale1 = scale1\n",
    "        self.scale2 = scale2\n",
    "        self.scale3 = scale3\n",
    "\n",
    "    def build_model(self):\n",
    "        input_data = Input(shape=self.input_shape)\n",
    "        stem = self.stem(input_data)\n",
    "        ira = Activation(\"relu\")(stem)\n",
    "        for i in range(5):\n",
    "            ira = self.IRA(ira)\n",
    "\n",
    "        ra = self.RA(ira)\n",
    "\n",
    "        irb = Activation(\"relu\")(ra)\n",
    "        for i in range(10):\n",
    "            irb = self.IRB(irb)\n",
    "\n",
    "        rb = self.RB(irb)\n",
    "\n",
    "        irc = Activation(\"relu\")(rb)\n",
    "        for i in range(5):\n",
    "            irc = self.IRC(irc)\n",
    "\n",
    "        pool = GlobalAveragePooling2D()(irc)\n",
    "\n",
    "        drop = Dropout(0.2)(pool)\n",
    "        final = Dense(self.n_classes, activation= self.activation)(drop)\n",
    "        model = Model(inputs = input_data, outputs = final)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def stem(self, input_data):\n",
    "        conv_1 = Conv2D(32, (3,3), strides=2, padding=\"valid\")(input_data)\n",
    "        conv_1 = BatchNormalization()(conv_1)\n",
    "        conv_1 = Activation('relu')(conv_1)\n",
    "\n",
    "        conv_2 = Conv2D(32, (3, 3), padding=\"valid\")(conv_1)\n",
    "        conv_2 = BatchNormalization()(conv_2)\n",
    "        conv_2 = Activation('relu')(conv_2)\n",
    "\n",
    "        conv_3 = Conv2D(64, (3, 3), padding=\"same\")(conv_2)\n",
    "        conv_3 = BatchNormalization()(conv_3)\n",
    "        conv_3 = Activation('relu')(conv_3)\n",
    "\n",
    "        pool_1 = MaxPool2D((3,3), strides=2, padding='valid')(conv_3)\n",
    "\n",
    "        conv_4 = Conv2D(96, (3, 3), strides = 2, padding=\"valid\")(conv_3)\n",
    "        conv_4 = BatchNormalization()(conv_4)\n",
    "        conv_4 = Activation('relu')(conv_4)\n",
    "\n",
    "        conc_1 = concatenate([pool_1, conv_4])\n",
    "\n",
    "        conv_5_1 = Conv2D(64, (1, 1), padding=\"same\")(conc_1)\n",
    "        conv_5_1 = BatchNormalization()(conv_5_1)\n",
    "        conv_5_1 = Activation('relu')(conv_5_1)\n",
    "\n",
    "        conv_6_1 = Conv2D(96, (3, 3), padding=\"valid\")(conv_5_1)\n",
    "        conv_6_1 = BatchNormalization()(conv_6_1)\n",
    "        conv_6_1 = Activation('relu')(conv_6_1)\n",
    "\n",
    "        conv_5_2 = Conv2D(64, (1, 1), padding=\"same\")(conc_1)\n",
    "        conv_5_2 = BatchNormalization()(conv_5_2)\n",
    "        conv_5_2 = Activation('relu')(conv_5_2)\n",
    "\n",
    "        conv_6_2 = Conv2D(64, (7, 1), padding=\"same\")(conv_5_2)\n",
    "        conv_6_2 = BatchNormalization()(conv_6_2)\n",
    "        conv_6_2 = Activation('relu')(conv_6_2)\n",
    "\n",
    "        conv_7_2 = Conv2D(64, (1, 7), padding=\"same\")(conv_6_2)\n",
    "        conv_7_2 = BatchNormalization()(conv_7_2)\n",
    "        conv_7_2 = Activation('relu')(conv_7_2)\n",
    "\n",
    "        conv_8_2 = Conv2D(96, (3, 3), padding=\"valid\")(conv_7_2)\n",
    "        conv_8_2 = BatchNormalization()(conv_8_2)\n",
    "        conv_8_2 = Activation('relu')(conv_8_2)\n",
    "\n",
    "        conc_2 = concatenate([conv_6_1, conv_8_2])\n",
    "\n",
    "        conv_1_3 = Conv2D(192, (3,3),strides=2, padding=\"valid\")(conc_2)\n",
    "        conv_1_3 = BatchNormalization()(conv_1_3)\n",
    "        conv_1_3 = Activation(\"relu\")(conv_1_3)\n",
    "\n",
    "        pool_1_4 = MaxPool2D(strides=2, padding = \"valid\")(conc_2)\n",
    "\n",
    "        conc_3 = concatenate([conv_1_3, pool_1_4])\n",
    "\n",
    "        return conc_3\n",
    "\n",
    "    #fig 16\n",
    "    def IRA(self, input_data):\n",
    "        # relu_1 = Activation(\"relu\")(input_data)\n",
    "        b_0 = Conv2D(32, (1,1), padding=\"same\")(input_data)\n",
    "        b_0 = BatchNormalization()(b_0)\n",
    "        b_0 = Activation(\"relu\")(b_0)\n",
    "\n",
    "        b_1 = Conv2D(32, (1, 1), padding=\"same\")(input_data)\n",
    "        b_1 = BatchNormalization()(b_1)\n",
    "        b_1 = Activation(\"relu\")(b_1)\n",
    "        b_1 = Conv2D(32, (3, 3), padding=\"same\")(b_1)\n",
    "        b_1 = BatchNormalization()(b_1)\n",
    "        b_1 = Activation(\"relu\")(b_1)\n",
    "\n",
    "        b_2 = Conv2D(32, (1, 1), padding=\"same\")(input_data)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "        b_2 = Conv2D(48, (3, 3), padding=\"same\")(b_2)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "        b_2 = Conv2D(64, (3, 3), padding=\"same\")(b_2)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "\n",
    "        conc_1 = concatenate([b_0, b_1, b_2])\n",
    "\n",
    "        conv = Conv2D(384, (1,1), padding=\"same\")(conc_1)\n",
    "\n",
    "        out = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n",
    "               output_shape=K.int_shape(conv)[1:],\n",
    "               arguments={'scale': self.scale1})([input_data, conv])\n",
    "        out = Activation(\"relu\")(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # fig 7\n",
    "    # k l m n = 256 256 384 384\n",
    "    def RA(self, input_data):\n",
    "        b_0 = MaxPool2D((3,3), strides=2, padding=\"valid\")(input_data)\n",
    "\n",
    "        b_1 = Conv2D(384, (3,3), strides=2, padding=\"valid\")(input_data)\n",
    "        b_1 = BatchNormalization()(b_1)\n",
    "        b_1 = Activation(\"relu\")(b_1)\n",
    "\n",
    "        b_2 = Conv2D(256, (1,1), padding=\"same\")(input_data)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "        b_2 = Conv2D(256, (3,3), padding=\"same\")(b_2)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "        b_2 = Conv2D(384, (3,3), strides=2, padding=\"valid\")(b_2)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "\n",
    "        conc_1 = concatenate([b_0, b_1, b_2])\n",
    "\n",
    "        return conc_1\n",
    "\n",
    "    # fig 17\n",
    "    def IRB(self, input_data):\n",
    "        b_1 = Conv2D(192, (1,1), padding=\"same\")(input_data)\n",
    "        b_1 = BatchNormalization()(b_1)\n",
    "        b_1 = Activation(\"relu\")(b_1)\n",
    "\n",
    "        b_2 = Conv2D(128, (1,1), padding=\"same\")(input_data)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "        b_2 = Conv2D(160, (1,7), padding=\"same\")(b_2)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "        b_2 = Conv2D(192, (7,1), padding=\"same\")(b_2)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "\n",
    "        conc_1 = concatenate([b_1, b_2])\n",
    "\n",
    "        conv = Conv2D(1152, (1,1), padding=\"same\")(conc_1)\n",
    "\n",
    "        out = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n",
    "               output_shape=K.int_shape(conv)[1:],\n",
    "               arguments={'scale': self.scale2})([input_data, conv])\n",
    "        out = Activation(\"relu\")(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def RB(self, input_data):\n",
    "        b_0 = MaxPool2D((3,3), strides=2, padding=\"valid\")(input_data)\n",
    "\n",
    "        b_1 = Conv2D(256, (1,1), padding=\"same\")(input_data)\n",
    "        b_1 = BatchNormalization()(b_1)\n",
    "        b_1 = Activation(\"relu\")(b_1)\n",
    "        b_1 = Conv2D(384, (3,3),strides=2, padding=\"valid\")(b_1)\n",
    "        b_1 = BatchNormalization()(b_1)\n",
    "        b_1 = Activation(\"relu\")(b_1)\n",
    "\n",
    "        b_2 = Conv2D(256, (1,1), padding=\"same\")(input_data)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "        b_2 = Conv2D(288, (3,3),strides=2, padding=\"valid\")(b_2)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "\n",
    "        b_3 = Conv2D(256, (1,1), padding=\"same\")(input_data)\n",
    "        b_3 = BatchNormalization()(b_3)\n",
    "        b_3 = Activation(\"relu\")(b_3)\n",
    "        b_3 = Conv2D(288, (3,3), padding=\"same\")(b_3)\n",
    "        b_3 = BatchNormalization()(b_3)\n",
    "        b_3 = Activation(\"relu\")(b_3)\n",
    "        b_3 = Conv2D(320, (3,3),strides=2, padding=\"valid\")(b_3)\n",
    "        b_3 = BatchNormalization()(b_3)\n",
    "        b_3 = Activation(\"relu\")(b_3)\n",
    "\n",
    "        conc = concatenate([b_0,b_1,b_2,b_3])\n",
    "        return conc\n",
    "\n",
    "    def IRC(self, input_data):\n",
    "        b_1 = Conv2D(192, (1,1), padding=\"same\")(input_data)\n",
    "        b_1 = BatchNormalization()(b_1)\n",
    "        b_1 = Activation(\"relu\")(b_1)\n",
    "\n",
    "        b_2 = Conv2D(192, (1,1), padding=\"same\")(input_data)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "        b_2 = Conv2D(224, (1,3), padding=\"same\")(b_2)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "        b_2 = Conv2D(256, (3,1), padding=\"same\")(b_2)\n",
    "        b_2 = BatchNormalization()(b_2)\n",
    "        b_2 = Activation(\"relu\")(b_2)\n",
    "\n",
    "        conc_1 = concatenate([b_1, b_2])\n",
    "\n",
    "        conv = Conv2D(2144, (1,1), padding=\"same\")(conc_1)\n",
    "        out = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n",
    "               output_shape=K.int_shape(conv)[1:],\n",
    "               arguments={'scale': self.scale3})([input_data, conv])\n",
    "        relu = Activation(\"relu\")(out)\n",
    "        return relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkUK5BfiAqPL",
    "outputId": "d681accc-2d83-4602-fb78-05eb726f0d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "107/107 [==============================] - 75s 474ms/step - loss: 2.9534 - accuracy: 0.3170 - val_loss: 6.4341 - val_accuracy: 0.2781\n",
      "Epoch 2/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 2.2020 - accuracy: 0.3935 - val_loss: 2.2731 - val_accuracy: 0.3822\n",
      "Epoch 3/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 1.9868 - accuracy: 0.4372 - val_loss: 2.2162 - val_accuracy: 0.3885\n",
      "Epoch 4/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 1.7019 - accuracy: 0.5095 - val_loss: 1.7550 - val_accuracy: 0.4947\n",
      "Epoch 5/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 1.3840 - accuracy: 0.5921 - val_loss: 2.2389 - val_accuracy: 0.4565\n",
      "Epoch 6/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 1.1519 - accuracy: 0.6516 - val_loss: 2.1337 - val_accuracy: 0.4756\n",
      "Epoch 7/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.9103 - accuracy: 0.7278 - val_loss: 1.4898 - val_accuracy: 0.5966\n",
      "Epoch 8/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.7521 - accuracy: 0.7700 - val_loss: 0.8687 - val_accuracy: 0.7537\n",
      "Epoch 9/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.5804 - accuracy: 0.8134 - val_loss: 1.3036 - val_accuracy: 0.6539\n",
      "Epoch 10/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.4716 - accuracy: 0.8503 - val_loss: 1.0961 - val_accuracy: 0.6964\n",
      "Epoch 11/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.4069 - accuracy: 0.8734 - val_loss: 1.2859 - val_accuracy: 0.6688\n",
      "Epoch 12/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.3080 - accuracy: 0.8992 - val_loss: 0.6841 - val_accuracy: 0.8068\n",
      "Epoch 13/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.2620 - accuracy: 0.9124 - val_loss: 0.6145 - val_accuracy: 0.8259\n",
      "Epoch 14/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.2522 - accuracy: 0.9171 - val_loss: 0.6904 - val_accuracy: 0.8068\n",
      "Epoch 15/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.2372 - accuracy: 0.9194 - val_loss: 0.8060 - val_accuracy: 0.7856\n",
      "Epoch 16/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.2161 - accuracy: 0.9276 - val_loss: 0.7333 - val_accuracy: 0.8025\n",
      "Epoch 17/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.1685 - accuracy: 0.9385 - val_loss: 0.9273 - val_accuracy: 0.7686\n",
      "Epoch 18/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.1916 - accuracy: 0.9326 - val_loss: 0.9007 - val_accuracy: 0.7431\n",
      "Epoch 19/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.1961 - accuracy: 0.9329 - val_loss: 0.7753 - val_accuracy: 0.7834\n",
      "Epoch 20/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.1663 - accuracy: 0.9396 - val_loss: 0.6938 - val_accuracy: 0.8323\n",
      "Epoch 21/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.1709 - accuracy: 0.9420 - val_loss: 0.6625 - val_accuracy: 0.8301\n",
      "Epoch 22/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.1575 - accuracy: 0.9435 - val_loss: 0.6136 - val_accuracy: 0.8280\n",
      "Epoch 23/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.1318 - accuracy: 0.9511 - val_loss: 0.5970 - val_accuracy: 0.8450\n",
      "Epoch 24/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.1295 - accuracy: 0.9522 - val_loss: 0.4923 - val_accuracy: 0.8577\n",
      "Epoch 25/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.1180 - accuracy: 0.9531 - val_loss: 0.6003 - val_accuracy: 0.8556\n",
      "Epoch 26/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.1154 - accuracy: 0.9561 - val_loss: 0.5228 - val_accuracy: 0.8662\n",
      "Epoch 27/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.1143 - accuracy: 0.9587 - val_loss: 0.7101 - val_accuracy: 0.8344\n",
      "Epoch 28/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.1259 - accuracy: 0.9508 - val_loss: 1.1969 - val_accuracy: 0.7282\n",
      "Epoch 29/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.1219 - accuracy: 0.9558 - val_loss: 1.3787 - val_accuracy: 0.7240\n",
      "Epoch 30/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.1370 - accuracy: 0.9508 - val_loss: 0.7032 - val_accuracy: 0.8450\n",
      "Epoch 31/40\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.1560 - accuracy: 0.9440 - val_loss: 0.8425 - val_accuracy: 0.7898\n",
      "Epoch 32/40\n",
      "107/107 [==============================] - 46s 431ms/step - loss: 0.1454 - accuracy: 0.9493 - val_loss: 0.8041 - val_accuracy: 0.8301\n",
      "Epoch 33/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.1266 - accuracy: 0.9528 - val_loss: 0.5534 - val_accuracy: 0.8386\n",
      "Epoch 34/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.1271 - accuracy: 0.9517 - val_loss: 0.8120 - val_accuracy: 0.8089\n",
      "Epoch 35/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.1110 - accuracy: 0.9534 - val_loss: 0.5776 - val_accuracy: 0.8726\n",
      "Epoch 36/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.0825 - accuracy: 0.9619 - val_loss: 0.6347 - val_accuracy: 0.8684\n",
      "Epoch 37/40\n",
      "107/107 [==============================] - 46s 431ms/step - loss: 0.0853 - accuracy: 0.9619 - val_loss: 0.5852 - val_accuracy: 0.8705\n",
      "Epoch 38/40\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.0807 - accuracy: 0.9616 - val_loss: 0.6499 - val_accuracy: 0.8599\n",
      "Epoch 39/40\n",
      "107/107 [==============================] - 46s 431ms/step - loss: 0.0925 - accuracy: 0.9581 - val_loss: 0.8682 - val_accuracy: 0.7983\n",
      "Epoch 40/40\n",
      "107/107 [==============================] - 46s 431ms/step - loss: 0.1048 - accuracy: 0.9581 - val_loss: 0.6150 - val_accuracy: 0.8493\n",
      "0.1 0.2 0.15 0.8726114630699158\n"
     ]
    }
   ],
   "source": [
    "#11:30\n",
    "from numba import cuda\n",
    "import gc\n",
    "test_scales = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "test = []\n",
    "model = []\n",
    "i, j, k = 0.1, 0.2, 0.15\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "# tf.keras.backend.set_learning_phase(True)\n",
    "model = IRV2(X_train.shape[1:], 48, activation= 'softmax', scale1=i, scale2=j, scale3=k).build_model()\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\",\n",
    "                metrics=['accuracy'])\n",
    "epochs = 40\n",
    "history = model.fit(X_train, y_train_e, batch_size=32, epochs=epochs, validation_data=(X_val, y_val_e))\n",
    "max_h = max(history.history['val_accuracy'])\n",
    "test.append(str(i) + ' ' + str(j) + ' ' + str(k) + ' ' + str(max_h))\n",
    "print(str(i) + ' ' + str(j) + ' ' + str(k) + ' ' + str(max_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-Yz4tA8QQiZ",
    "outputId": "06826601-7b3b-4fc5-c5c9-01d53175bae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880\n"
     ]
    }
   ],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NxbC_HoC8zS",
    "outputId": "616262ac-9bbe-4d4e-b105-5ffd8d30e108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.1 0.2\n",
      "0.1 0.1 0.25\n",
      "0.1 0.1 0.3\n",
      "0.1 0.15 0.1\n",
      "0.1 0.15 0.15\n",
      "0.1 0.15 0.2\n",
      "0.1 0.15 0.25\n",
      "0.1 0.15 0.3\n",
      "0.1 0.2 0.1\n",
      "0.1 0.2 0.15\n",
      "0.1 0.2 0.2\n",
      "0.1 0.2 0.25\n",
      "0.1 0.2 0.3\n",
      "0.1 0.25 0.1\n",
      "0.1 0.25 0.15\n",
      "0.1 0.25 0.2\n",
      "0.1 0.25 0.25\n",
      "0.1 0.25 0.3\n",
      "0.1 0.3 0.1\n",
      "0.1 0.3 0.15\n",
      "0.1 0.3 0.2\n",
      "0.1 0.3 0.25\n",
      "0.1 0.3 0.3\n",
      "0.15 0.1 0.1\n",
      "0.15 0.1 0.15\n",
      "0.15 0.1 0.2\n",
      "0.15 0.1 0.25\n",
      "0.15 0.1 0.3\n",
      "0.15 0.15 0.1\n",
      "0.15 0.15 0.15\n",
      "0.15 0.15 0.2\n",
      "0.15 0.15 0.25\n",
      "0.15 0.15 0.3\n",
      "0.15 0.2 0.1\n",
      "0.15 0.2 0.15\n",
      "0.15 0.2 0.2\n",
      "0.15 0.2 0.25\n",
      "0.15 0.2 0.3\n",
      "0.15 0.25 0.1\n",
      "0.15 0.25 0.15\n",
      "0.15 0.25 0.2\n",
      "0.15 0.25 0.25\n",
      "0.15 0.25 0.3\n",
      "0.15 0.3 0.1\n",
      "0.15 0.3 0.15\n",
      "0.15 0.3 0.2\n",
      "0.15 0.3 0.25\n",
      "0.15 0.3 0.3\n",
      "0.2 0.1 0.1\n",
      "0.2 0.1 0.15\n",
      "0.2 0.1 0.2\n",
      "0.2 0.1 0.25\n",
      "0.2 0.1 0.3\n",
      "0.2 0.15 0.1\n",
      "0.2 0.15 0.15\n",
      "0.2 0.15 0.2\n",
      "0.2 0.15 0.25\n",
      "0.2 0.15 0.3\n",
      "0.2 0.2 0.1\n",
      "0.2 0.2 0.15\n",
      "0.2 0.2 0.2\n",
      "0.2 0.2 0.25\n",
      "0.2 0.2 0.3\n",
      "0.2 0.25 0.1\n",
      "0.2 0.25 0.15\n",
      "0.2 0.25 0.2\n",
      "0.2 0.25 0.25\n",
      "0.2 0.25 0.3\n",
      "0.2 0.3 0.1\n",
      "0.2 0.3 0.15\n",
      "0.2 0.3 0.2\n",
      "0.2 0.3 0.25\n",
      "0.2 0.3 0.3\n",
      "0.25 0.1 0.1\n",
      "0.25 0.1 0.15\n",
      "0.25 0.1 0.2\n",
      "0.25 0.1 0.25\n",
      "0.25 0.1 0.3\n",
      "0.25 0.15 0.1\n",
      "0.25 0.15 0.15\n",
      "0.25 0.15 0.2\n",
      "0.25 0.15 0.25\n",
      "0.25 0.15 0.3\n",
      "0.25 0.2 0.1\n",
      "0.25 0.2 0.15\n",
      "0.25 0.2 0.2\n",
      "0.25 0.2 0.25\n",
      "0.25 0.2 0.3\n",
      "0.25 0.25 0.1\n",
      "0.25 0.25 0.15\n",
      "0.25 0.25 0.2\n",
      "0.25 0.25 0.25\n",
      "0.25 0.25 0.3\n",
      "0.25 0.3 0.1\n",
      "0.25 0.3 0.15\n",
      "0.25 0.3 0.2\n",
      "0.25 0.3 0.25\n",
      "0.25 0.3 0.3\n",
      "0.3 0.1 0.1\n",
      "0.3 0.1 0.15\n",
      "0.3 0.1 0.2\n",
      "0.3 0.1 0.25\n",
      "0.3 0.1 0.3\n",
      "0.3 0.15 0.1\n",
      "0.3 0.15 0.15\n",
      "0.3 0.15 0.2\n",
      "0.3 0.15 0.25\n",
      "0.3 0.15 0.3\n",
      "0.3 0.2 0.1\n",
      "0.3 0.2 0.15\n",
      "0.3 0.2 0.2\n",
      "0.3 0.2 0.25\n",
      "0.3 0.2 0.3\n",
      "0.3 0.25 0.1\n",
      "0.3 0.25 0.15\n",
      "0.3 0.25 0.2\n",
      "0.3 0.25 0.25\n",
      "0.3 0.25 0.3\n",
      "0.3 0.3 0.1\n",
      "0.3 0.3 0.15\n",
      "0.3 0.3 0.2\n",
      "0.3 0.3 0.25\n",
      "0.3 0.3 0.3\n"
     ]
    }
   ],
   "source": [
    "test_scales = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "test = []\n",
    "for i in test_scales:\n",
    "    for j in test_scales:\n",
    "        for k in test_scales:\n",
    "            if i != 0.1 or j != 0.1 or (k != 0.1 and k != 0.15):\n",
    "                print(str(i) + ' ' + str(j) + ' ' + str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2m1XDdnYXL-",
    "outputId": "d6037985-8d8f-40be-8054-837b0c10d51f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "138/138 [==============================] - 101s 516ms/step - loss: 2.7708 - accuracy: 0.3179 - val_loss: 87.5530 - val_accuracy: 0.1353\n",
      "Epoch 2/40\n",
      "138/138 [==============================] - 64s 462ms/step - loss: 2.1505 - accuracy: 0.3938 - val_loss: 2.0749 - val_accuracy: 0.4099\n",
      "Epoch 3/40\n",
      "138/138 [==============================] - 64s 462ms/step - loss: 1.8565 - accuracy: 0.4675 - val_loss: 1.8978 - val_accuracy: 0.4793\n",
      "Epoch 4/40\n",
      "138/138 [==============================] - 64s 461ms/step - loss: 1.5847 - accuracy: 0.5312 - val_loss: 2.5118 - val_accuracy: 0.4024\n",
      "Epoch 5/40\n",
      "138/138 [==============================] - 64s 461ms/step - loss: 1.2351 - accuracy: 0.6320 - val_loss: 1.2971 - val_accuracy: 0.6071\n",
      "Epoch 6/40\n",
      "138/138 [==============================] - 64s 461ms/step - loss: 0.9691 - accuracy: 0.7050 - val_loss: 1.4337 - val_accuracy: 0.6003\n",
      "Epoch 7/40\n",
      "138/138 [==============================] - 64s 461ms/step - loss: 0.7225 - accuracy: 0.7779 - val_loss: 1.6697 - val_accuracy: 0.5316\n",
      "Epoch 8/40\n",
      "138/138 [==============================] - 64s 460ms/step - loss: 0.5814 - accuracy: 0.8108 - val_loss: 1.2949 - val_accuracy: 0.6642\n",
      "Epoch 9/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.4426 - accuracy: 0.8602 - val_loss: 0.5833 - val_accuracy: 0.8368\n",
      "Epoch 10/40\n",
      "138/138 [==============================] - 64s 461ms/step - loss: 0.3718 - accuracy: 0.8804 - val_loss: 0.7010 - val_accuracy: 0.8015\n",
      "Epoch 11/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.3004 - accuracy: 0.9026 - val_loss: 1.0771 - val_accuracy: 0.6961\n",
      "Epoch 12/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.2918 - accuracy: 0.9046 - val_loss: 0.6567 - val_accuracy: 0.8137\n",
      "Epoch 13/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.2471 - accuracy: 0.9182 - val_loss: 0.5130 - val_accuracy: 0.8498\n",
      "Epoch 14/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.2170 - accuracy: 0.9232 - val_loss: 0.4435 - val_accuracy: 0.8776\n",
      "Epoch 15/40\n",
      "138/138 [==============================] - 64s 461ms/step - loss: 0.2018 - accuracy: 0.9279 - val_loss: 0.6935 - val_accuracy: 0.8035\n",
      "Epoch 16/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1999 - accuracy: 0.9311 - val_loss: 0.7650 - val_accuracy: 0.7831\n",
      "Epoch 17/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1951 - accuracy: 0.9295 - val_loss: 0.7603 - val_accuracy: 0.7920\n",
      "Epoch 18/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1813 - accuracy: 0.9309 - val_loss: 0.6521 - val_accuracy: 0.8253\n",
      "Epoch 19/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1722 - accuracy: 0.9341 - val_loss: 0.6267 - val_accuracy: 0.8266\n",
      "Epoch 20/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1625 - accuracy: 0.9395 - val_loss: 0.6772 - val_accuracy: 0.8226\n",
      "Epoch 21/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1551 - accuracy: 0.9395 - val_loss: 0.4961 - val_accuracy: 0.8606\n",
      "Epoch 22/40\n",
      "138/138 [==============================] - 64s 461ms/step - loss: 0.1592 - accuracy: 0.9343 - val_loss: 0.9281 - val_accuracy: 0.7492\n",
      "Epoch 23/40\n",
      "138/138 [==============================] - 64s 461ms/step - loss: 0.1504 - accuracy: 0.9372 - val_loss: 0.7180 - val_accuracy: 0.8131\n",
      "Epoch 24/40\n",
      "138/138 [==============================] - 64s 461ms/step - loss: 0.1639 - accuracy: 0.9370 - val_loss: 0.6439 - val_accuracy: 0.8144\n",
      "Epoch 25/40\n",
      "138/138 [==============================] - 64s 460ms/step - loss: 0.1464 - accuracy: 0.9436 - val_loss: 0.5372 - val_accuracy: 0.8545\n",
      "Epoch 26/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1348 - accuracy: 0.9436 - val_loss: 0.5934 - val_accuracy: 0.8423\n",
      "Epoch 27/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1319 - accuracy: 0.9479 - val_loss: 0.6529 - val_accuracy: 0.8402\n",
      "Epoch 28/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1289 - accuracy: 0.9438 - val_loss: 0.4900 - val_accuracy: 0.8634\n",
      "Epoch 29/40\n",
      "138/138 [==============================] - 64s 461ms/step - loss: 0.1202 - accuracy: 0.9497 - val_loss: 0.6597 - val_accuracy: 0.8192\n",
      "Epoch 30/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1345 - accuracy: 0.9458 - val_loss: 0.6003 - val_accuracy: 0.8375\n",
      "Epoch 31/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1459 - accuracy: 0.9411 - val_loss: 0.6611 - val_accuracy: 0.8266\n",
      "Epoch 32/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1620 - accuracy: 0.9366 - val_loss: 1.7139 - val_accuracy: 0.6635\n",
      "Epoch 33/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1419 - accuracy: 0.9413 - val_loss: 0.4910 - val_accuracy: 0.8749\n",
      "Epoch 34/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1129 - accuracy: 0.9483 - val_loss: 0.5482 - val_accuracy: 0.8586\n",
      "Epoch 35/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1170 - accuracy: 0.9440 - val_loss: 0.6357 - val_accuracy: 0.8396\n",
      "Epoch 36/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1091 - accuracy: 0.9495 - val_loss: 0.5447 - val_accuracy: 0.8688\n",
      "Epoch 37/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1163 - accuracy: 0.9477 - val_loss: 0.5828 - val_accuracy: 0.8681\n",
      "Epoch 38/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1027 - accuracy: 0.9529 - val_loss: 0.6284 - val_accuracy: 0.8586\n",
      "Epoch 39/40\n",
      "138/138 [==============================] - 63s 460ms/step - loss: 0.1068 - accuracy: 0.9515 - val_loss: 0.5130 - val_accuracy: 0.8586\n",
      "Epoch 40/40\n",
      "138/138 [==============================] - 64s 461ms/step - loss: 0.1024 - accuracy: 0.9520 - val_loss: 0.5566 - val_accuracy: 0.8634\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model = IRV2(X_train.shape[1:], 48, activation= 'softmax').build_model()\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "epochs = 40\n",
    "history = model.fit(X_train, y_train_e, batch_size=32, epochs=epochs, validation_data=(X_val, y_val_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDxqLSnRCGE9",
    "outputId": "b9d67fe0-2f7b-4dd9-be12-265bfeefe3f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8776342868804932\n"
     ]
    }
   ],
   "source": [
    "print(max(history.history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ht45Wk-YE6lR",
    "outputId": "7cf707b0-8bc2-4b87-e3c7-5e25032343a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: IRv2/assets\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_ff689642-2f41-4066-aa7a-77d0e1271db1\", \"history_IRv2_70.json\", 2877)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import pandas as pd\n",
    "model.save(\"IRv2\")\n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "# save to json:  \n",
    "hist_json_file = 'history_IRv2_70.json' \n",
    "with open(hist_json_file, mode='w') as f:\n",
    "    hist_df.to_json(f)\n",
    "\n",
    "files.download('history_IRv2_70.json')\n",
    "#files.download('IRv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNCl9mB8z0yY"
   },
   "outputs": [],
   "source": [
    "model = IRV2(X_train.shape[1:], 47, activation= 'softmax').build_model()\n",
    "# decay = .9 e = 1.0 lr = 0.045, exp rate = 0.94\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=tf.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJeMrTO5z7mB",
    "outputId": "cda7d6cf-c9d6-4c7b-d2bc-d90799120f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "138/138 [==============================] - 31s 125ms/step - loss: 2.9496 - accuracy: 0.2343 - val_loss: 2.6825 - val_accuracy: 0.2726 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 2.6389 - accuracy: 0.2853 - val_loss: 2.4061 - val_accuracy: 0.3623 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 2.4183 - accuracy: 0.3435 - val_loss: 2.1711 - val_accuracy: 0.4310 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 2.1904 - accuracy: 0.3923 - val_loss: 1.9650 - val_accuracy: 0.4602 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "138/138 [==============================] - 13s 96ms/step - loss: 1.9691 - accuracy: 0.4521 - val_loss: 1.7649 - val_accuracy: 0.5337 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 1.7344 - accuracy: 0.5037 - val_loss: 1.5537 - val_accuracy: 0.5812 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 1.5851 - accuracy: 0.5531 - val_loss: 1.4082 - val_accuracy: 0.6288 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 1.3889 - accuracy: 0.5917 - val_loss: 1.2221 - val_accuracy: 0.6744 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 1.2411 - accuracy: 0.6447 - val_loss: 1.1187 - val_accuracy: 0.6785 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 1.0677 - accuracy: 0.6964 - val_loss: 0.9932 - val_accuracy: 0.7315 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "138/138 [==============================] - 13s 94ms/step - loss: 0.9639 - accuracy: 0.7242 - val_loss: 0.8906 - val_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.8419 - accuracy: 0.7589 - val_loss: 0.8258 - val_accuracy: 0.7641 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.7771 - accuracy: 0.7791 - val_loss: 0.7726 - val_accuracy: 0.7906 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.6708 - accuracy: 0.8010 - val_loss: 0.7164 - val_accuracy: 0.7859 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.6089 - accuracy: 0.8180 - val_loss: 0.7143 - val_accuracy: 0.7981 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.5442 - accuracy: 0.8375 - val_loss: 0.6259 - val_accuracy: 0.8192 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.5070 - accuracy: 0.8545 - val_loss: 0.6367 - val_accuracy: 0.8205 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.4650 - accuracy: 0.8600 - val_loss: 0.5924 - val_accuracy: 0.8239 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.4350 - accuracy: 0.8715 - val_loss: 0.5771 - val_accuracy: 0.8219 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.3950 - accuracy: 0.8815 - val_loss: 0.5391 - val_accuracy: 0.8389 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.3814 - accuracy: 0.8856 - val_loss: 0.5382 - val_accuracy: 0.8423 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.3432 - accuracy: 0.8953 - val_loss: 0.5462 - val_accuracy: 0.8266 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.3381 - accuracy: 0.8980 - val_loss: 0.5354 - val_accuracy: 0.8382 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.3314 - accuracy: 0.8983 - val_loss: 0.5244 - val_accuracy: 0.8382 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.3005 - accuracy: 0.9044 - val_loss: 0.5022 - val_accuracy: 0.8525 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.3087 - accuracy: 0.8996 - val_loss: 0.4803 - val_accuracy: 0.8504 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.2792 - accuracy: 0.9119 - val_loss: 0.4937 - val_accuracy: 0.8457 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.2799 - accuracy: 0.9125 - val_loss: 0.4831 - val_accuracy: 0.8511 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.2559 - accuracy: 0.9153 - val_loss: 0.4843 - val_accuracy: 0.8532 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.2590 - accuracy: 0.9196 - val_loss: 0.4693 - val_accuracy: 0.8538 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.2533 - accuracy: 0.9198 - val_loss: 0.5021 - val_accuracy: 0.8498 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.2493 - accuracy: 0.9148 - val_loss: 0.4830 - val_accuracy: 0.8627 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.2354 - accuracy: 0.9223 - val_loss: 0.5057 - val_accuracy: 0.8430 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.2373 - accuracy: 0.9218 - val_loss: 0.4934 - val_accuracy: 0.8450 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "138/138 [==============================] - 13s 94ms/step - loss: 0.2244 - accuracy: 0.9252 - val_loss: 0.4727 - val_accuracy: 0.8620 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.2245 - accuracy: 0.9234 - val_loss: 0.5037 - val_accuracy: 0.8504 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.2264 - accuracy: 0.9214\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.2264 - accuracy: 0.9214 - val_loss: 0.4838 - val_accuracy: 0.8538 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.2033 - accuracy: 0.9311 - val_loss: 0.4746 - val_accuracy: 0.8579 - lr: 1.0000e-04\n",
      "Epoch 39/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.1878 - accuracy: 0.9370 - val_loss: 0.4723 - val_accuracy: 0.8566 - lr: 1.0000e-04\n",
      "Epoch 40/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.1925 - accuracy: 0.9293 - val_loss: 0.4723 - val_accuracy: 0.8600 - lr: 1.0000e-04\n",
      "Epoch 41/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.1827 - accuracy: 0.9350 - val_loss: 0.4733 - val_accuracy: 0.8593 - lr: 1.0000e-04\n",
      "Epoch 42/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.1871 - accuracy: 0.9336 - val_loss: 0.4727 - val_accuracy: 0.8552 - lr: 1.0000e-04\n",
      "Epoch 43/50\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.9370\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.1741 - accuracy: 0.9370 - val_loss: 0.4769 - val_accuracy: 0.8559 - lr: 1.0000e-04\n",
      "Epoch 44/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.1674 - accuracy: 0.9409 - val_loss: 0.4765 - val_accuracy: 0.8552 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.1778 - accuracy: 0.9368 - val_loss: 0.4762 - val_accuracy: 0.8552 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "138/138 [==============================] - 13s 94ms/step - loss: 0.1712 - accuracy: 0.9384 - val_loss: 0.4764 - val_accuracy: 0.8559 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.1707 - accuracy: 0.9361 - val_loss: 0.4761 - val_accuracy: 0.8552 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.1695 - accuracy: 0.9395 - val_loss: 0.4758 - val_accuracy: 0.8552 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9390\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.1747 - accuracy: 0.9390 - val_loss: 0.4757 - val_accuracy: 0.8545 - lr: 1.0000e-05\n",
      "Epoch 50/50\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.1686 - accuracy: 0.9411 - val_loss: 0.4757 - val_accuracy: 0.8538 - lr: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, ZeroPadding2D, BatchNormalization\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(96,(11,11),strides=4,activation='relu',input_shape=X_train.shape[1:],padding='valid'))\n",
    "model.add(MaxPool2D((3,3),strides=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(ZeroPadding2D(padding=2))\n",
    "model.add(Conv2D(256,(5,5),activation='relu',padding='same'))\n",
    "model.add(MaxPool2D((3,3), strides=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(384,(3,3),activation='relu',padding='same'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(384,(3,3),activation='relu',padding='same'))\n",
    "\n",
    "model.add(ZeroPadding2D(padding=1))\n",
    "model.add(Conv2D(256,(3,3),activation='relu',padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096,activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(4096,activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(48,activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=tf.optimizers.SGD(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "#callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.1, patience=5, verbose=1,\n",
    "    mode='max', min_delta=0.0001, cooldown=2, min_lr=0)\n",
    "epochs = 50\n",
    "history = model.fit(X_train, y_train_e, batch_size=32, epochs=epochs,\n",
    "validation_data=(X_val, y_val_e), callbacks = [callback])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "InveptionResNetv2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
