{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_c, out_c, size, stride, padding):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_c, out_c, kernel_size=size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_c, eps=0.001, momentum=0.1, affine=True)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mixed_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixed_1, self).__init__()\n",
    "        self.branch0 = Conv(64, 96, 3, stride=2, padding = \"valid\")\n",
    "        self.pool = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            Conv(160, 64, 1, stride=1, padding=\"same\"),\n",
    "            Conv(64, 96, 3, stride=1, padding=\"valid\")\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            Conv(160, 64, 1, stride=1, padding=\"same\"),\n",
    "            Conv(64, 64, (7,1), stride=1, padding=\"same\"),\n",
    "            Conv(64, 64, (1,7), stride=1, padding=\"same\"),\n",
    "            Conv(64, 96, 3, stride=1, padding=\"valid\")\n",
    "        )\n",
    "\n",
    "        self.conv_1 = Conv(192, 192, 3, stride = 1, padding=\"valid\")\n",
    "        self.pool_1 = nn.MaxPool2d(3, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pool = self.pool(x)\n",
    "        x0 = self.branch0(x)\n",
    "        out = torch.cat((pool, x0), 1) #128\n",
    "        b1 = self.branch1(out) #96\n",
    "        b2 = self.branch2(out) #96\n",
    "        out = torch.cat((b1,b2),1) #192\n",
    "        conv = self.conv_1(out)\n",
    "        pool = self.pool_1(out)\n",
    "        print(conv.shape, pool.shape, out.shape)\n",
    "        out = torch.cat((conv, pool),1)\n",
    "        return out #384\n",
    "\n",
    "\n",
    "class IRA(nn.Module):\n",
    "    def __init__(self, scale):\n",
    "        super(IRA, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.branch_0 = Conv(384, 32, stride=1, size=1, padding=\"same\")\n",
    "        self.branch_1 = nn.Sequential(\n",
    "            Conv(384, 32, size=1, stride=1, padding=\"same\"),\n",
    "            Conv(32, 32, size=3, stride=1, padding=\"same\")\n",
    "        )\n",
    "        self.branch_2 = nn.Sequential(\n",
    "            Conv(384, 32, size=1, stride=1, padding=\"same\"),\n",
    "            Conv(32, 48, size=3, stride=1, padding=\"same\"),\n",
    "            Conv(48, 64, size=3, stride=1, padding=\"same\")\n",
    "        )\n",
    "        self.conv = nn.Conv2d(160, 384, kernel_size=1, stride=1, padding=\"same\")\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1 = self.branch_0(x)\n",
    "        x2 = self.branch_2(x)\n",
    "        x3 = self.branch_2(x)\n",
    "        conc = torch.cat((x1, x2, x3),1)\n",
    "        out = self.conv(conc)\n",
    "        out = out * self.scale + x\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class RA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RA, self).__init__()\n",
    "        self.branch_0 = nn.MaxPool2d((2,2), stride=2)\n",
    "        self.branch_1 = Conv(384, 384, size=3, stride=2, padding = \"valid\")\n",
    "        self.branch_2 = nn.Sequential(\n",
    "            Conv(384, 256, size=1, stride=1, padding=\"same\"),\n",
    "            Conv(256, 256, size=3, stride=1, padding=\"same\"),\n",
    "            Conv(256, 384, size=3, stride=2, padding = \"valid\")\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch_0(x)\n",
    "        x1 = self.branch_1(x)\n",
    "        x2 = self.branch_2(x)\n",
    "        out = torch.cat((x0, x1, x2), 1)\n",
    "        return out #1152\n",
    "\n",
    "class IRB(nn.Module):\n",
    "    def __init__(self, scale):\n",
    "        super(IRB, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.branch_0 = Conv(1152, 192, stride = 1, size=1, padding=\"same\")\n",
    "        self.branch_1 = nn.Sequential(\n",
    "            Conv(1152, 128, size=1, stride=1, padding=\"same\"),\n",
    "            Conv(128, 160, size=(7,1), stride=1, padding=\"same\"),\n",
    "            Conv(160, 192, size=(1,7), stride=1, padding=\"same\")\n",
    "        )\n",
    "        self.conv = nn.Conv2d(384, 1152, kernel_size=1, stride=1, padding=\"same\")\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch_0(x)\n",
    "        x1 = self.branch_1(x)\n",
    "        out = torch.cat((x0,x1), 1)\n",
    "        out = self.conv(out)\n",
    "        out = out * self.scale + x\n",
    "        out = self.relu(out)\n",
    "        return out #1152\n",
    "\n",
    "class RB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RB, self).__init__()\n",
    "        self.branch_0 = nn.MaxPool2d(3, stride=2)\n",
    "        self.branch_1 = nn.Sequential(\n",
    "            Conv(1152, 256, size=1, stride=1, padding=\"same\"),\n",
    "            Conv(256, 384, size=3, stride=2, padding=\"valid\")\n",
    "        )\n",
    "        self.branch_2 = nn.Sequential(\n",
    "            Conv(1152, 256, size=1, stride=1, padding=\"same\"),\n",
    "            Conv(256, 288, size=3, stride=2, padding=\"valid\")\n",
    "        )\n",
    "        self.branch_3 = nn.Sequential(\n",
    "            Conv(1152, 256, size=1, stride=1, padding=\"same\"),\n",
    "            Conv(256, 288, size=3, stride=1, padding=\"same\"),\n",
    "            Conv(288, 320, size=3, stride=2, padding=\"valid\")\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch_0(x)\n",
    "        x1 = self.branch_1(x)\n",
    "        x2 = self.branch_2(x)\n",
    "        x3 = self.branch_3(x)\n",
    "        out = torch.cat((x0,x1,x2,x3), 1)\n",
    "        return out #2144\n",
    "\n",
    "class IRC(nn.Module):\n",
    "    def __init__(self, scale):\n",
    "        super(IRC, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.branch_0 = Conv(2144, 192, stride = 1, size=1, padding=\"same\")\n",
    "        self.branch_1 = nn.Sequential(\n",
    "            Conv(2144, 192, size=1, stride=1, padding=\"same\"),\n",
    "            Conv(192, 224, size=(1,3), stride=1, padding=\"same\"),\n",
    "            Conv(224, 256, size=(3,1), stride=1, padding=\"same\")\n",
    "        )\n",
    "        self.conv = nn.Conv2d(448, 2144, kernel_size=1, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch_0\n",
    "        x1 = self.branch_1\n",
    "        out = torch.cat((x0,x1), 1)\n",
    "        out = self.conv(out)\n",
    "        out = out * self.scale + x\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class IRV2(nn.Module):\n",
    "    def __init__(self, input_shape, n_classes, scale1, scale2, scale3):\n",
    "        super(IRV2, self,).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.scale1 = scale1\n",
    "        self.scale2 = scale2\n",
    "        self.scale3 = scale3\n",
    "        # modules\n",
    "        self.relu = nn.ReLU(inplace = False)\n",
    "        self.conv_1 = Conv(3, 32, size=3, stride=2, padding=\"valid\")\n",
    "        self.conv_2 = Conv(32, 32, size=3, stride=1, padding=\"valid\")\n",
    "        self.conv_3 = Conv(32, 64, size=3, stride=1, padding=\"same\")\n",
    "        self.mixed_1 = Mixed_1()\n",
    "        self.ira = nn.Sequential(\n",
    "            IRA(scale=scale1),\n",
    "            IRA(scale=scale1),\n",
    "            IRA(scale=scale1),\n",
    "            IRA(scale=scale1),\n",
    "            IRA(scale=scale1),\n",
    "        )\n",
    "        self.ra = RA()\n",
    "        self.irb = nn.Sequential(\n",
    "            IRB(scale=scale2),\n",
    "            IRB(scale=scale2),\n",
    "            IRB(scale=scale2),\n",
    "            IRB(scale=scale2),\n",
    "            IRB(scale=scale2),\n",
    "            IRB(scale=scale2),\n",
    "            IRB(scale=scale2),\n",
    "            IRB(scale=scale2),\n",
    "            IRB(scale=scale2),\n",
    "            IRB(scale=scale2),\n",
    "        )\n",
    "        self.rb = RB()\n",
    "        self.irc = nn.Sequential(\n",
    "            IRC(scale=scale3),\n",
    "            IRC(scale=scale3),\n",
    "            IRC(scale=scale3),\n",
    "            IRC(scale=scale3),\n",
    "            IRC(scale=scale3),\n",
    "        )\n",
    "        self.pool = nn.AvgPool2d(8, count_include_pad=False)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.final = nn.Linear(2144, n_classes)\n",
    "\n",
    "    def features(self, input1):\n",
    "        print(type(input1))\n",
    "        x = self.conv_1(input1)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.mixed_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.ira(x)\n",
    "        x = self.ra(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.irb(x)\n",
    "        x = self.rb(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.irc(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, features):\n",
    "        x = self.pool(features)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = self.logits(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"lib/datasets/train1/X_train1.npy\")[1000:]\n",
    "X_val = np.load(\"lib/datasets/train1/X_val1.npy\")[1000:]\n",
    "y_train_e = np.load(\"lib/datasets/train1/y_train_e1.npy\")[1000:]\n",
    "y_val_e = np.load(\"lib/datasets/train1/y_val_e1.npy\" )[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_target = torch.from_numpy(y_train_e)\n",
    "val_target = torch.from_numpy(y_val_e)\n",
    "train = torch.from_numpy(np.moveaxis(X_train, -1, 1).astype(np.float32))\n",
    "val = torch.from_numpy(X_val)\n",
    "train_tensor = torch.utils.data.TensorDataset(train, train_target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "val_tensor = torch.utils.data.TensorDataset(val, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3413, 299, 299, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3413])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3413, 3, 299, 299)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.moveaxis(X_train, -1, 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 299, 299])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 192, 69, 69]) torch.Size([32, 192, 69, 69]) torch.Size([32, 192, 71, 71])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 12.00 GiB total capacity; 9.81 GiB already allocated; 0 bytes free; 10.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16936\\17940127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Forward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16936\\2506224365.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16936\\2506224365.py\u001b[0m in \u001b[0;36mfeatures\u001b[1;34m(self, input1)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mra\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mirb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16936\\2506224365.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m \u001b[1;31m#1152\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 12.00 GiB total capacity; 9.81 GiB already allocated; 0 bytes free; 10.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "input_shape = X_train.shape[1:]\n",
    "n_classes = 48\n",
    "model = IRV2(input_shape=input_shape, n_classes=n_classes, scale1=0.1, scale2=0.1, scale3=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 40\n",
    "model.to(device)\n",
    "for epoch in range(epochs):\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        # Obtaining the cuda parameters\n",
    "        print(data.shape)\n",
    "        data = data.to(device=device)\n",
    "        target = target.to(device=device)\n",
    "\n",
    "        # Reshaping to suit our model\n",
    "\n",
    "        # Forward propagation\n",
    "        score = model(data)\n",
    "        print(score)\n",
    "        loss = criterion(score, target)\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
